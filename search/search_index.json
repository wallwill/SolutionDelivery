{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Solution Delivery Documentation","text":"<p>The following documentation covers:</p> <ul> <li>Solution Architecture Process (Generic)</li> <li>Solution Architecture Review Process (Generic)</li> </ul>"},{"location":"adrs/","title":"Decision Records","text":"<p>An Architecture Decision Record (ADR) is a document that captures an important architectural decision made along with its context and consequences. Overarching principle is that ADRs should be context aware and demonstrate due diligence and critical analysis has been applied.</p> <p>For the Enterprise , ADRs will give visibility to decisions that are typically made in meetings without record. As time passes, these are quickly forgotten as to the why of the meeting and who made them.   If implemented in such a way as to be correlated to entities of significance such as an Project ID or Application ID then reports can be generated which show the historical progression of an Architecture through its lifecycle and the decisions that went with it.</p> <p>For the Architects and Application Teams, it expedites decision making through federated accountability while keeping technical decisions of significance closer to the project and execution teams. </p> <p>Github.com Examples</p>"},{"location":"assesment/","title":"Scoring Criteria for Peer Review","text":""},{"location":"assesment/#general-guidelines","title":"General Guidelines","text":"<ul> <li>If a program exhibits criteria from multiple ratings on the scale, review members should use their judgment to determine which criteria are more important for the architecture being reviewed and provide a rating accordingly. Keep it simple and avoid over-engineering the scoring process.</li> <li>If any tech strategy is not applicable to the architecture/investment being reviewed, rate it as 3, the same as acceptable strategic alignment.</li> <li>Use these criteria as guidelines. Reviewers should use their best judgment to provide an objective rating for the architecture, as not every possible scenario might be covered within the criteria.</li> </ul>"},{"location":"assesment/#what-does-rate-1-5-signify","title":"What Does Rate 1-5 Signify?","text":"Rating Significance 1 Lacks Strategic Alignment 2 Deviates from Strategy 3 Acceptable Strategic Alignment 4 Good Strategic Alignment 5 Best Strategic Alignment"},{"location":"assesment/#modernization","title":"Modernization","text":"Score Criteria 1 - New or unmodified monolithic application - Application does not scale - Architecture inhibits ability to make core data accessible through data externalization techniques - Not aligned to the criteria for Modern APIs and API standards with a critical severity rating of &gt;3000ms P95 latency - No AI best practices met 2 - New or unmodified monolithic application - Application scales vertically - No plans to make core data accessible through data externalization techniques. - Somewhat aligned to the criteria for Modern, API standards with a high severity rating of &gt;1000ms but &lt;3000ms P95 latency - 1-5 AI best practices met; team has no plan to meet outstanding 3 - Minor enhancements to monolithic application, majority microservices cloud architecture. - Majority cloud native architecture, Application scales both vertically and horizontally with manual intervention - Plans in place to make core data accessible through data externalization techniques. - Targeted to align to the criteria for Modern API standards with a medium severity rating of &gt;500ms but &lt;1000ms P95 latency - 1-5 AI best practices met, team has plan to meet outstanding or 6-10 AI best practices met, team has no plan to meet outstanding 4 - Transitioning towards a full microservices, cloud-based architecture - Using CI/CD, automation for build/deploy but testing not fully automated - Transitioning towards full cloud native architecture, Application scales both vertically and horizontally with partial automation - Core data accessible through data externalization techniques - Aligned to the criteria for Modern API standards with a low severity rating of &gt;200ms but &lt;500ms P95 latency soon to be reported within Enterprise Advisor - 6-10 AI best practices met, team has plan to meet outstanding 5 - Cloud-based architecture (Cloud native and failure mgmt. Containers. Based on Domain Driven Design and Microservice Architecture, Infra as code) - Full cloud native architecture, Application scales both vertically and horizontally with full automation - Recognized as key contributor, maintainer and/or reviewer for 3rd party contributions Core data accessible through data externalization techniques, aligned with and promoting use of Enterprise core architecture - Building technology asset using emerging technologies for enterprise reuse - Aligned to the criteria for Modern API standards with a goal rating of &lt;200ms P95 latency as reported by Enterprise Advisor - All AI best practices met"},{"location":"assesment/#reuse-simplify","title":"Reuse &amp; Simplify","text":"Score Criteria 1 - Increase in TCO and addition to technical or architectural debt - Not a core API, internal API solving one business use case - No reuse of core API or core asset - Duplication of function and/or data of an existing Enterprise Capability but for a different data set/domain/sales context - No method of publishing capability backlog (Aha!) - No contribution to inner source - No code/config/lifecycle segregation of Line of Business (LoB) specific functionality from core common capabilities - Uses Discouraged or Unacceptable technologies - Significantly adds to the duplication of data, duplication of code bases, and/or duplication of security-related context/attributes - No metadata or capability publication to enterprise solutions 2 - Inherits existing technical or architecture debt with minor incremental additions to debt - Not a core API, internal API solving multiple business use cases - No reuse of core API or core asset - No contribution to inner source - Transitioning to publishing product backlog - Limited duplication of data, no duplication/forking of code, no duplication of security-related context/attributes - No metadata or capability publication to enterprise solutions 3 - Inherits existing technical or architecture debt with minor incremental additions to debt - Not a core API, internal API solving multiple business use cases - No reuse of core API or core asset - No contribution to inner source - Transitioning to publishing product backlog - Limited duplication of data, no duplication/forking of code, no duplication of security-related context/attributes - Limited metadata publication to enterprise solutions 4 - Significant (or net) reduction to technical or architectural debt, OR significant reductions in duplication of existing function - Not a core API, internal API solving multiple business use cases - Core enterprise asset - Reuse of core API - Contributing to inner source - Transition towards exposing data and functionality as API streams, not all sources streaming - Transition towards exposing/consuming product/application agnostic Business Events to the Enterprise ecosystem - Primarily leveraging Preferred technologies 5 - Decommissioning of existing assets directly reducing Total Cost of Ownership and Technical Debt - Is a core API, can be reused by other applications - Core enterprise asset - Producing and contributing to streams - No duplication of data from Enterprise capabilities - Product/Application agnostic Business Events published to Enterprise Consumers - Contribution to inner source - Fully matured federated product and federated software development lifecycle - Fully enterprise viewable federated product capability backlog with regular cadence of product council review, prioritization, and feature governance"},{"location":"assesment/#data-management","title":"Data Management","text":"<p>(Note: Assumption here of an Enterprise Data Catalog)</p> Score Criteria 1 - Data Governance: Data usage and rights have been mainly ignored. What's a data steward? Business owner/stakeholders are not at the table for decisions. Unnecessarily duplicates data (tables) and repositories. Datasets (raw metadata) do not exist in the Enterprise Catalog. - Metadata Management: There is no capture of any metadata in the solution. - Data Quality: Data quality characteristics are not understood for data repositories present in this solution. No DQ exception processing has been considered. Enterprise capability is ignored, and actions are taken based on inputs from consumer and downstream applications. - Data Orchestration &amp; Integration: Job scheduling and pipeline orchestration have not been considered. Existing Data Products have not been considered with no intent to build a new one. The data pipeline uses unacceptable tools. Failure points and impacts on downstream processes undefined. Requirements for data pipeline latency, throughput, and elasticity are not defined. No RPO or RTO defined. - Information Model: Missed opportunities to employ canonical data; no roadmap to remediate; accesses source system data directly. - Data Privacy: Data at rest or in transit is never encrypted, or sensitive data (PHI/PII) is not masked/de-identified/tokenized. Enterprise capability is ignored. - Reporting &amp; Analytics: Solution ignores reporting and analytics considerations. - Data Repositories: Scalability, reliability, serviceability, performance, fault-tolerance, availability, and capacity for the solution are ignored. Solution fails when data repositories are unavailable. Site availability less than 80%. No auto scalability, unmet SLAs, no DR or forecast plan. - Reference Data: Reference data traditionally ignored, leading to duplicate, outdated, and inaccurate data instances with multiple licenses, incurring cost. 2 - Data Governance: Datasets with raw metadata are available on the Enterprise Data Catalog with enforced metadata by contract. Some data usage and rights criteria reviewed. Ad-hoc data steward available. Sporadic stakeholder involvement. Some duplicative outcome for the enterprise. - Metadata Management: Capture of existing production technical or business metadata exists in a non-preferred data catalog. - Data Quality: Data quality characteristics understood for major repositories; minor DQ exception processing considered. Enterprise capabilities acknowledged but not engaged. Focus is on ETL-based DQ check or manual QA team queries. - Data Orchestration &amp; Integration: Minimal job scheduling and pipeline orchestration, with discouraged tools. Some failure points addressed. Requirements for latency, throughput, and elasticity partially defined. RPO and RTO defined but viability unclear. - Information Model: Missed opportunities to employ canonical data; remediation roadmap in place; accesses source system data directly. 3 - Data Governance: Datasets with raw metadata and some enrichments available on the Enterprise Catalog. Informally reviewed data usage and rights criteria. Ad-hoc data steward available with some stakeholder processes. Less duplicative outcome for the enterprise. - Metadata Management: Existing production technical metadata used to populate a dataset in the Enterprise Data Platform with some business metadata (e.g., data steward, definitions). - Data Privacy: Sensitive data at rest or in transit is masked/de-identified/tokenized. Enterprise capabilities assessed. - Data Orchestration &amp; Integration: Minor job scheduling and orchestration with minor automation. Some existing Data Products leveraged. Pipeline uses acceptable tools. Most failure points addressed. Defined latency, throughput, and elasticity requirements, albeit viability is unclear. - Information Model: Some use of canonical data where appropriate; does not access source system data directly. - Data Quality: Some data quality characteristics understood for some repositories. Some DQ processing considered. Enterprise capabilities evaluated. - Reporting &amp; Analytics: Solution considers major reporting and analytics elements. Enterprise capabilities assessed. - Data Repositories: Some focus on scalability, reliability, and other metrics. Site availability 85-90%. Auto-scaling exists with generally met SLAs. Some performance metrics captured periodically, with DR and forecast planning in place. - Reference Data: Datasets available on the Enterprise Data Catalog for public healthcare and endorsed business standards, including technical and business metadata for healthcare reference data licensed by other units. 4 - Data Governance: Asset available on the Enterprise Data Catalog as Data Product(s) with robust enrichments. Subscription requests reviewed within the Data Platform Catalog. Access manually provisioned. Minimum duplicative outcome for the enterprise. - Metadata Management: Production metadata used in a full-function suite in the Data Platform with data lineage and manual enrichment. - Data Quality: Well-understood data quality characteristics for key repositories. Happy-path DQ processing evident. Enterprise capabilities included in plan. - Data Orchestration &amp; Integration: Good job scheduling and automation support. Existing Data Products leveraged. Some preferred tools used. Most failure points addressed and recovery plans considered. Defined latency, throughput, and elasticity requirements. RPO and RTO plans developed. - Information Model: Good use of canonical data; does not access source system data directly. - Data Privacy: Sensitive data both at rest and in transit masked/de-identified/tokenized. Enterprise capabilities planned. - Reporting &amp; Analytics: Reporting and analytics integral to the solution, with enterprise capabilities planned. - Data Repositories: Significant attention to metrics like scalability and availability, surviving short-term outages. Numerous dashboards for metrics. Site availability 90-95%. Auto-scaling solutions mostly meeting SLAs with RCAs on discrepancies. - Reference Data: Subscription requests reviewed by Data Product Owners; reference data access managed by the reference management team via the HPC Data Platform. 5 - Data Governance: Asset available on the Enterprise Data Catalog as Data Product(s) with extensive metadata enrichments. - Metadata Management: Metadata populates a full-function suite with comprehensive lineage and automated enrichment using Enterprise classification tools. - Data Quality: Comprehensive understanding of data quality characteristics, including profiling and DQ exception processing. - Data Orchestration &amp; Integration: Automated job scheduling and orchestration. Existing and new reusable Data Products. Uses all preferred tools. All failure points addressed. Defined latency, throughput, and elasticity requirements with evidence of achievement, including RPO/RTO design. - Information Model: Excellent use of canonical data, strengthening existing definitions; avoids direct system data access. - Data Privacy: Sensitive data both at rest and in transit masked/de-identified/tokenized. Enterprise capabilities engaged as needed. - Data Integration, Reporting &amp; Analytics: Comprehensive account in architecture with engaged enterprise capabilities. - Data Repositories: Robust solutions with metrics captured via dashboards, monitoring, and alerts. All SLAs tracked, with comprehensive planning for scalability, fault-tolerance, and capacity. . - Reference Data : All subscription requests are reviewed within the Enterprise Data Platform Catalog by Data Product Owners."},{"location":"assesment/#security","title":"Security","text":"Score Criteria 1 - Less than 59% of applications use enterprise capability for identity and access management - Less than 59% of applications use enterprise capability for source code management and infrastructure as code - Less than 59% of applications use enterprise-approved encryption methods for data at rest and data in transit - Less than 59% of applications have logging and monitoring in place using enterprise capability - Less than 59% remediation of Severe, Critical, and High violations for all applicable vulnerabilities scans according to application risk category (Static, Dynamic, Pen Test) - Less than 59% of applications have appropriate network controls in place for segmentation, endpoint protection, and defense in depth 2 - 60% - 69% of applications use enterprise capability for identity and access management - 60% - 69% of applications use enterprise capability for source code management and infrastructure as code (GitHub) - 60% - 69% of applications use enterprise-approved encryption methods for data at rest and data in transit - 60% - 69% of applications have logging and monitoring in place using enterprise capability - 60% - 69% of remediations for Severe, Critical, and High violations for all applicable vulnerabilities scans according to application risk category (Static, Dynamic, Pen Test) - 60% - 69% of applications have appropriate network controls in place for segmentation, endpoint protection, and defense in depth 3 - 70% - 79% of applications use enterprise capability for identity and access management - 70% - 79% of applications use enterprise capability for source code management and infrastructure as code - 70% - 79% of applications use enterprise-approved encryption methods for data at rest and data in transit - 70% - 79% of applications have logging and monitoring in place using enterprise capability - 70% - 79% remediation of Severe, Critical, and High violations of all applicable vulnerabilities according to application risk category (Static, Dynamic, Pen Test) - 70% - 79% of applications have appropriate network controls in place for segmentation, endpoint protection, and defense in depth 4 - 80% - 90% of applications use enterprise capability for identity and access management - 80% - 90% of applications use enterprise capability for source code management and infrastructure as code - 80% - 90% of applications use enterprise-approved encryption methods for data at rest and data in transit - 80% - 90% of applications have logging and monitoring in place using enterprise capability - 80% - 90% remediation of Severe, Critical, and High violations of all applicable vulnerabilities according to application risk category (Static, Dynamic, Pen Test) - 80% - 90% of applications have appropriate network controls in place for segmentation, endpoint protection, and defense in depth 5 - 100% of applications use enterprise capability for identity and access management - 100% of applications use enterprise capability for source code management and infrastructure as code - 100% of applications use enterprise-approved encryption methods for data at rest and data in transit - 100% of applications have logging and monitoring in place using enterprise capability - 100% remediation of Severe, Critical, and High violations of all applicable vulnerabilities according to application risk category (Static, Dynamic, Pen Test) - 100% of applications have appropriate network controls in place for segmentation, endpoint protection, and defense in depth <pre><code>                                                                                                     |\n</code></pre>"},{"location":"assesment/#consumer-experience","title":"Consumer Experience","text":"Score Criteria 1 - Stakeholder experience is not seamless or personalized, leading to a negative NPS for the product or solution - Outreach to end constituents (e.g., via email, SMS) is not aligned to standards, outmoded (e.g., paper, fax), and is not documented or coordinated across business LoBs - Experiences are created in a bespoke fashion based on modality (e.g., web only), not allowing seamless leverage across various devices (mobile, tablet, etc.) - Significant increase to TCO in the short or long term with no new capabilities - Solution architecture and development process does not enable quick delivery of business value (more than 9-12 months) - Solution does not use common components to provide a consistent experience - Solution does not tag and track experience 2 - Stakeholder experience is less than seamless or personalized, potentially leading to a negative NPS for the product or solution - Outreach to end constituents (e.g., via email, SMS) is not aligned to standards and is not documented or coordinated across business LoBs - Experiences are not fully multimodal in nature - No impact to TCO - Solution architecture and development process enables delivery of business value (less than 9 months), but iteration speed to assess, measure, and adjust experience is slow (&gt;4 weeks) - Solution uses one or two common components to provide a consistent experience but could use more and has apparent experience deficiencies - Solution tags and tracks experience without using standard RUM and consumer journey tracking solutions 3 - Incremental experience design leading to a more personalized or seamless experience in the long-term, improving NPS in the long run - Outreach to end constituents (e.g., via email, SMS) is aligned to standards but may not be fully documented or coordinated across all business LoBs - Experiences are designed to be multimodal but may not be optimized for all modalities (e.g., sub-par mobile experience) - Reduces TCO in less than 18 months - Solution architecture and development process enables quick delivery of business value (less than 4 months), with moderate iteration speed (2-4 weeks) - Solution uses common components to provide a consistent experience but could use more, with minor experience deficiencies - Solution tags and tracks most of the experience using enterprise standard RUM and consumer journey tracking solutions 4 - Experience design leading to a more personalized or seamless experience in the short-term, improving NPS in the long run - Outreach to end constituents (e.g., via email, SMS) is aligned to standards, fully documented, and coordinated for a given business - Experiences are designed to be multimodal and optimized across modalities - Reduces TCO in the near future - Solution architecture and development process enables quick delivery of business value (less than 2 months), with quick iteration speed (&lt;2 weeks) - Solution uses common components to provide a consistent experience at every opportunity - Solution tags and tracks experience using enterprise standard RUM and consumer journey tracking solutions 5 - Experience design leading to a highly personalized or seamless experience in the immediate term, significantly improving NPS in the short run - Outreach to end constituents (e.g., via email, SMS) is aligned to standards, fully documented, and coordinated across the enterprise - Experiences are designed to be multimodal and optimized across modalities - Significantly reduces TCO in the near future - Solution architecture and development process enables quick turnaround (less than 1 month), with fast iteration speed (&lt;3 days) - Solution uses common components to provide a consistent experience at every opportunity - Solution tags and tracks experience using enterprise standard RUM and consumer journey tracking solutions <pre><code>   ## Engineering Excellence\n</code></pre> Score Criteria 1 - Engineering Culture: Less than 40% of program engineers contribute meaningful code in GitHub over a 30-day period - Engineering Culture: Less than 60% of eligible program source code repositories are in GitHub Enterprise Cloud - Performance &amp; Accountability: Less than 30% of applications have respective Vital Business Functions (VBF) identified, mapped, monitored with availability probes, and integrated into Enterprise Advisor for reporting - Open Source Software and Technology: &lt;30% of technologies implemented are Preferred open source solutions - Open Source technologies are not registered to the applications or compliant with licensing - Program team has not contributed to any open source projects leveraged in the solutions 2 - Engineering Culture: 40-49% of program engineers contribute meaningful code in GitHub over a 30-day period - Engineering Culture: 60% of eligible program source code repositories are in GitHub Enterprise Cloud - Performance &amp; Accountability: 30% or more applications have respective VBF identified, mapped, monitored with availability probes - Open Source Software and Technology: 30% of technologies implemented are Preferred open source solutions - Open Source technologies are not registered to the applications or compliant with licensing - Program team has not contributed code, docs, or feedback to the open source projects leveraged in the solutions 3 - Engineering Culture: 50-54% of program engineers contribute meaningful code in GitHub over a 30-day period - Engineering Culture: 70% of eligible program source code repositories are in GitHub Enterprise Cloud - Performance &amp; Accountability: 40% or more applications have respective VBF identified, mapped, monitored with availability probes, and integrated into Enterprise Advisor for reporting - Open Source Software and Technology: 50% of technologies implemented are Preferred open source solutions - Open Source technologies are registered to the applications or program in Barista and compliant with licensing - Program team has contributed code, docs, or feedback to one or more open source projects leveraged in the solutions 4 - Engineering Culture: 55-59% of program engineers contribute meaningful code in GitHub over a 30-day period - Engineering Culture: 80% of eligible program source code repositories are in GitHub Enterprise Cloud - Performance &amp; Accountability: 50% or more applications have respective VBF identified, mapped, monitored with availability probes, and integrated into Enterprise Advisor for reporting - Open Source Software and Technology: 65% of technologies implemented are Preferred open source solutions - Open Source technologies are registered to the applications and compliant with licensing - Aspects of the program are released as open source - Program team has contributed code or docs to one or more open source projects leveraged in the solution(s) 5 - Engineering Culture: 60% of program engineers contribute meaningful code in GitHub over a 30-day period - Engineering Culture: 90% of eligible program source code repositories are in GitHub Enterprise Cloud - Performance &amp; Accountability: 60% or more applications have respective VBF identified, mapped, monitored with availability probes - Open Source Software and Technology: 80% of technologies implemented are Preferred open source solutions - Open Source technologies are registered to the applications and compliant with licensing - Aspects of the program are released as open source - Program team has contributed code, docs, and feedback to one or more open source projects leveraged in the solution(s)"},{"location":"communications/","title":"Communications","text":""},{"location":"communications/#objectives-alignment","title":"Objectives Alignment","text":"<p>Ensure alignment of communication efforts with the overarching goals of the Architecture Ireland group, focusing on promoting alignment between business and technology strategies and facilitating efficient solution delivery, while considering the global audience's needs.</p>"},{"location":"communications/#audience-segmentation","title":"Audience Segmentation","text":"<p>Recognize the diversity within the global audience, considering different time zones, cultural backgrounds, and communication preferences. Segment the audience into:</p> <ul> <li>Executives responsible for business and technology strategy</li> <li>Delivery teams involved in solution implementation</li> </ul>"},{"location":"communications/#platform-selection","title":"Platform Selection","text":"<ul> <li>SharePoint: Use SharePoint, where appropriate, as a centralized repository for EA artifacts relating to IP/R&amp;D, ensuring accessibility to all global teams. Organize information logically and provide clear navigation for easy access.</li> <li>Confluence Wiki: Utilize Confluence Wiki for collaborative documentation, enabling teams across time zones to contribute asynchronously. Foster a culture of knowledge sharing and transparency.</li> <li>Microsoft Teams: Leverage Teams for real-time collaboration and discussions, but be mindful of scheduling meetings across time zones. Encourage the use of asynchronous communication features such as chat and threaded discussions.</li> <li>Email: Reserve email for formal communications and important updates, considering the appropriate timing to reach a global audience effectively.</li> </ul>"},{"location":"communications/#timing-considerations","title":"Timing Considerations","text":"<ul> <li>Schedule synchronous meetings and real-time discussions at times that accommodate the majority of the global audience, rotating meeting times periodically to ensure inclusivity.</li> <li>Encourage asynchronous communication for non-urgent matters, allowing team members to contribute and respond at their convenience.</li> </ul>"},{"location":"communications/#when-to-use-each-platform","title":"When to Use Each Platform","text":"<ul> <li>SharePoint and Confluence Wiki: Provide 24/7 access to documentation and resources, allowing teams to collaborate and access information at any time suitable for them.</li> <li>Microsoft Teams: Utilize for both synchronous and asynchronous communication, leveraging features such as chat, channels, and threaded discussions to facilitate ongoing collaboration across time zones.</li> <li>Email: Use email for formal announcements, updates, and important notifications, considering the timing to reach the widest possible audience effectively.</li> </ul>"},{"location":"communications/#why-use-each-platform","title":"Why Use Each Platform","text":"<ul> <li>SharePoint and Confluence Wiki: Serve as accessible repositories for documentation and resources, enabling global teams to collaborate and access information at their convenience.</li> <li>Microsoft Teams: Facilitate real-time collaboration and discussions, bridging the gap between geographically dispersed teams and fostering a sense of connectedness.</li> <li>Email: Provide a formal channel for important communications, ensuring that critical updates reach all stakeholders regardless of time zone differences.</li> </ul>"},{"location":"communications/#pull-versus-push-strategies","title":"Pull versus Push Strategies","text":"<p>Encourage a balanced approach between pull and push strategies, allowing team members to access information on SharePoint and Confluence Wiki at their convenience while using Microsoft Teams and email for timely updates and notifications.</p>"},{"location":"communications/#training-and-support","title":"Training and Support","text":"<p>Provide training and resources on effective asynchronous communication and time management techniques, emphasizing the importance of inclusivity and collaboration across time zones.</p>"},{"location":"communications/#feedback-and-iteration","title":"Feedback and Iteration","text":"<p>Solicit feedback from the global audience regularly to evaluate the effectiveness of the communication strategy, making adjustments as needed to better accommodate the diverse needs of teams across different time zones.</p>"},{"location":"diagrams/","title":"Diagram Standards","text":"<p>Inherit any Enterprise standards. However, these standards are often high-level, and implementation teams require more granularity with respect to standards and guidelines.</p> <p>This page will be a living document that advocates for diagrams we will produce for each scope.</p>"},{"location":"diagrams/#theme-4-rs","title":"Theme: 4 R's","text":"<ul> <li>Right Diagram</li> <li>Right Audience</li> <li>Right Channel/Method</li> <li>Right Time</li> </ul>"},{"location":"diagrams/#diagram-families","title":"Diagram Families","text":"<p>Selecting the right diagram is a skill in itself. There are several diagram families to choose from. The most frequently used are listed below. Here in CGS Ireland, we rely on the skill of our architects to judge the right diagram type for the right audience that conveys the right level of information at the right time. (See 4 R's above)</p> <p>Note: This is not meant to be a prescriptive list, but rather a listing of the most frequently used diagrams.</p>"},{"location":"diagrams/#c4","title":"C4","text":"<ul> <li>System Context Diagram</li> <li>Container Diagram</li> </ul>"},{"location":"diagrams/#uml","title":"UML","text":"<ul> <li>Component Diagram</li> <li>Sequence Diagram</li> </ul>"},{"location":"diagrams/#er","title":"ER","text":""},{"location":"diagrams/#dfd","title":"DFD","text":"<ul> <li>Level 0</li> <li>Level 1</li> </ul>"},{"location":"diagrams/#cloud-diagrams","title":"Cloud Diagrams","text":"<ul> <li>Hybrid</li> <li>Context Dependent</li> </ul>"},{"location":"diagrams/#audiences","title":"Audiences","text":"<ul> <li>US Leadership</li> <li>Irish Leadership</li> <li>Product</li> <li>Data Science</li> <li>Engineering</li> <li>Software Engineers</li> <li>Data Engineers</li> </ul>"},{"location":"diagrams/#published-diagram-must-haves","title":"Published Diagram Must-Haves","text":"<ul> <li>Legend</li> <li>Standard Icons (if CSP)</li> <li>Version history: (v0..n), Date, Author</li> <li>All flows/connectors numbered and footnotes of explanations &amp; change history, where appropriate to do so.</li> <li>If published to Confluence, be published using the Gliffy integration.</li> </ul>"},{"location":"duties/","title":"Scope and Responsibilities - General","text":"<ul> <li>Collaborate with Product Owners/PMs: Define functional and non-functional requirements and prioritize backlog.</li> <li>Collaborate with Business and Architects: Work with solution/enterprise architects to translate business requirements into scalable solution options.</li> <li>Collaborate with Data SMEs: Produce efficient data engineering pipelines.</li> <li>System Analysis: Analyze needs and requirements of existing and proposed systems; develop architectural artifacts for as-is, transitional, and future states.</li> <li>Operational Recommendations: Make recommendations to improve existing solutions in performance, stability, usability, and scalability.</li> <li>Facilitation of Proofs of Concept: Conduct PoCs to evaluate and suggest new technology opportunities (including open source) affecting solution delivery and customer satisfaction.</li> <li>Data Analysis Partnership: Partner with SMEs in data analysis to solve problems and proactively identify and resolve data issues.</li> <li>Enterprise Groups Collaboration: Align with best practices and governance in cloud, responsible AI, data usage rights, and technology governance.</li> </ul> <p>General Outputs:</p> <ul> <li>Architectural Artifacts</li> <li>Architectural Decision Records</li> </ul>"},{"location":"duties/#scope-and-responsibilities-detailed","title":"Scope and Responsibilities \u2013 Detailed","text":""},{"location":"duties/#enterprise-architecture-awareness-and-engagement","title":"Enterprise Architecture (Awareness and Engagement)","text":"<ul> <li>Domain and Data Model Governance</li> <li>Technology and Tools Governance</li> <li>Program Roadmap Alignment</li> </ul>"},{"location":"duties/#cloud-architecture-awareness-and-engagement","title":"Cloud Architecture (Awareness and Engagement)","text":"<ul> <li>Cloud platform setup and governance</li> <li>Cloud Tenant/account/technology/cost governance</li> <li>Multi-Team Cloud Support</li> </ul>"},{"location":"duties/#solution-architecture-actively-involved","title":"Solution Architecture (Actively Involved)","text":"<ul> <li>Review of application domain models, data management, non-functional requirements, security, metrics, monitoring, operational support, and logging</li> <li>System component design and integration, technology and tools identification</li> <li>Architecture blueprints, technical reviews, Proofs of Concept (PoCs), product roadmap alignment</li> </ul>"},{"location":"duties/#product-management-passive-reviewer","title":"Product Management (Passive Reviewer)","text":"<ul> <li>Product Roadmap Definition</li> <li>Use Case Definition</li> <li>Product Solutioning, UI/UX</li> </ul>"},{"location":"duties/#testing-passive-reviewer","title":"Testing (Passive Reviewer)","text":"<ul> <li>Test Plan and Acceptance Criteria</li> <li>Regression and Performance Test Report</li> <li>Quality Assurance</li> </ul>"},{"location":"duties/#development-guiding-role","title":"Development (Guiding Role)","text":"<ul> <li>Data and application/API component design and review</li> <li>Continuous integration, test, and delivery with automated pipelines</li> <li>Code quality and best practices</li> </ul>"},{"location":"duties/#devsecops-passive","title":"DevSecOps (Passive)","text":"<ul> <li>Packaging and release of software</li> <li>Security Governance</li> </ul>"},{"location":"duties/#appendix","title":"Appendix","text":""},{"location":"duties/#levels-of-granularity","title":"Levels of Granularity","text":""},{"location":"duties/#architecture","title":"Architecture","text":"<ul> <li>Enterprise Architecture, Program Roadmap</li> <li>Cloud Architecture</li> <li>Solution Architecture, Product Roadmap</li> <li>Scrum of Scrums, ART</li> </ul>"},{"location":"duties/#product","title":"Product","text":"<ul> <li>Program and Product Planning</li> <li>Requirements gathering and definition</li> <li>Sprint Planning and Sprint Demo review</li> <li>Product sign-off</li> </ul>"},{"location":"duties/#test","title":"Test","text":"<ul> <li>Test Planning</li> <li>Documentation and guides for test tools and testing best practices</li> <li>Integration and regression test execution</li> <li>Regression test sign-off</li> </ul>"},{"location":"duties/#development","title":"Development","text":"<ul> <li>Application and DevOps architecture, component, product and test design, release planning</li> <li>DevOps environment setup, provisioning, and monitoring</li> <li>Component build, deployment, unit and integration tests</li> <li>Daily Scrums, Sprint Planning and Refinement, Technical Reviews, Demos</li> </ul>"},{"location":"duties/#release","title":"Release","text":"<ul> <li>Release and Verification, Deployment Guide, Regression Tests</li> <li>Merge to Master and component versioning</li> <li>Operations and Support, Run Books</li> <li>Release Sign Off</li> </ul>"},{"location":"process/","title":"Solution Architecture Process","text":"<ul> <li>Overview</li> <li>Solution Architecture Delivery Process</li> <li>Overview: Solution Delivery Macro Process</li> <li>Phase: Solution Intake Process</li> <li>Phase: Solution Architecture Elaboration<ul> <li>Activity: E1 - Assess Strategic Architecture Gateway<ul> <li>Task: 01 - Assess EA Inputs</li> <li>Task: 02 - Understand Overall Value Streams</li> <li>Task: 03 - Understand Business Context</li> <li>Task: 04 - Understand Technical Landscape</li> <li>Task: 05 - Assess Innovation and IP value</li> </ul> </li> <li>Activity: E2 - Design Overall Solution<ul> <li>Task: 01 - Decompose Solution in Modules</li> <li>Task: 02 - Detail Overall Value Streams</li> <li>Task: 03 - Refine Overall Solution Design</li> <li>Task: 04 - Design Application Architecture</li> <li>Task: 05 - Design Data Architecture</li> <li>Task: 06 - Review Adherence to Enterprise Standards</li> <li>Task: 07 - Propose Spikes / Technical Enablers</li> </ul> </li> <li>Activity: E3 - Manage Stakeholders Communication<ul> <li>Task: 01 - Gather Stakeholder List</li> <li>Task: 02 - Establish Approval Chains</li> <li>Task: 03 - Establish Communication Channels</li> </ul> </li> <li>Activity: E4 - Manage Architecture Decisions<ul> <li>Task: 01 - Propose, Detail or Approve ADR's</li> </ul> </li> <li>Activity: E5 - Manage Risks and NFR's<ul> <li>Task: 01 - Manage Risks</li> <li>Task: 02 - Assess NFRs</li> <li>Task: 03 - Propose Spikes / Technical Enablers /ADR's</li> </ul> </li> <li>Activity: E6 - Manage Product Platform<ul> <li>Task: 01 - Support CI / CD</li> <li>Task: 02 - Support and Coordinate Observability</li> <li>Task: 03 - Coordinate with Devops</li> <li>Task: 04 - Support Test Automation</li> <li>Task: 05 - Optimise Product Resources</li> <li>Task: 06 - Support Performance Engineering</li> <li>Task: 07 - Propose Spikes / Technical Enablers /ADR's</li> <li>Task: 08 - Tag and Register Application in Platform Management Repos</li> <li>Task: 09 - Monitor and Manage Application Vulnerabilities</li> </ul> </li> <li>Activity: E7 - Manage Product Security and Compliance<ul> <li>Task: 01 - Submit Security Architect Report</li> <li>Task: 02 - Submit Data Governance Report</li> <li>Task: 03 - Submit Data Encryption Report</li> <li>Task: 04 - Submit Disaster Recovery Report</li> <li>Task: 05 - Submit User Lifecycle Management Report</li> <li>Task: 06 - Submit AI Report</li> <li>Task: 07 - Submit Application Security Report</li> <li>Task: 08 - Submit Change Management Report</li> <li>Task: 09 - Submit SDLC Report</li> </ul> </li> </ul> </li> <li>Phase: Feature Solution Delivery Process<ul> <li>Activity: F1 - Design Feature Solution Intent<ul> <li>Task: 01 - Run Impact Analysis</li> <li>Task: 02 - Collaborate with PO on System Use Cases / A/C</li> <li>Task: 03 - Collaborate with Application Architect / Tech Leaders</li> <li>Task: 04 - Collaborate with UX Designers</li> <li>Task: 05 - Address NFR's</li> <li>Task: 06 - Verify Adherence to Enterprise Standards</li> <li>Task: 07 - Review and Sign Off Application Architecture</li> <li>Task: 08 - Propose Spikes / Technical Enablers /ADR's</li> </ul> </li> <li>Activity: F2 - Manage Stakeholders</li> <li>Activity: F3 - Manage Architecture Decisions</li> <li>Activity: F4 - Risks and NFR's</li> <li>Activity: F5 - Manage Product Platform</li> <li>Activity: F6 - Submit Application for Security Review</li> </ul> </li> </ul>"}]}